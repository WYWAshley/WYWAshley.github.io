---
layout: post
title: Entropy
categories: [machine learning, Algorithm]
description: What's Entropy for?
keywords: Entropy

---

介绍各种熵的概念和意义

## 信息熵

**熵**的本质是**香农信息量**（ $log( \frac{1}{p})$，$-log(p)$ ）的期望。
熵在信息论中代表随机变量不确定度的度量。一个离散型随机变量 X 的熵 H(X) 定义为：
$$
H(X) =  - \sum\limits_{i = 1}^K {p({x_i})\log (p({x_i}))}
$$
一条信息的信息量大小和它的**不确定性**有直接的关系。我们需要搞清楚一件非常非常不确定的事，或者是我们一无所知的事，就需要了解大量的信息。相反，如果我们对某件事已经有了较多的了解，我们就不需要太多的信息就能把它搞清楚。所以，从这个角度，我们可以认为，**信息量的度量就等于不确定性的多少**。

考虑一个**离散**的随机变量 $x$，由上面两个例子可知，信息的量度应该依赖于概率分布 $p(x)$，因此我们想要寻找一个函数 $I(x)$，它是概率 $p(x)$ 的单调函数，表达了信息的内容。怎么寻找呢？如果我们有两个不相关的事件 $x$ 和 $y$，那么观察两个事件同时发生时获得的信息量应该等于观察到事件各自发生时获得的信息之和，即：$I(x,y)=I(x)+I(y)$ 

**因为两个事件是独立不相关的，因此 $p(x,y)=p(x)p(y)$。根据这两个关系，很容易看出 $I(x)$ 一定与 $p(x)$的对数有关，因为对数的运算法则是 $loga(mn)=logam+logan$。因此，我们有 $I(x)=−logp(x)$**其中负号是用来保证信息量是正数或者零。而 log 函数的底数选择是任意的**（信息论中基常常选择为2，因此信息的单位为比特 $ bits$；而机器学习中基常常选择为自然常数，因此单位常常被称为奈特 $nats$）。$I(x)$ 也被称为随机变量 $x$ 的**自信息 (self-information)，描述的是随机变量的某个事件发生所带来的信息量。图像如图：****

<img src="/images/posts/Entropy/1.png"/>

 

这里，我们就正式引出信息熵的概念。 现在假设一个发送者想传送一个随机变量的值给接收者。那么在这个过程中，他们传输的平均信息量可以通过求 $I(x)=−logp(x)$ 关于概率分布 $p(x)$ 的期望得到，即：
$$
H(X)=\sum \limits_{x} p(x)log{p(x)}=−\sum \limits_{i=1}^{n} p(xi)log{p(xi)}
$$
$H(X)$就被称为随机变量 $x$ 的**熵,它是表示随机变量不确定的度量，是对所有可能发生的事件产生的信息量的期望**。

从公式可得，**随机变量的取值个数越多，状态数也就越多，信息熵就越大，混乱程度就越大。当随机分布为均匀分布时，熵最大**，且 $0≤H(X)≤logn$。

<br>

## Shannon 编码定理

那么这些定义有着什么样的性质呢？考虑一个随机变量 $x$。这个随机变量有4种可能的状态，每个状态都是等可能的。为了把 $x$ 的值传给接收者，我们需要传输 $2bits$的消息。
$$
H(X)=−4×\frac{1}{4}log_2\frac{1}{4}=2\ bits
$$
现在考虑一个具有 4 种可能的状态 ${a,b,c,d}$ 的随机变量，每个状态各自的概率为 (12,14,18,18)

这种情形下的熵为：
$$
H(X)=−\frac{1}{2}log_2\frac{1}{2}−\frac{1}{4}log_2\frac{1}{4}−\frac{1}{8}log_2\frac{1}{8}−\frac{1}{8}log_2\frac{1}{8}=1.75\ bits
$$
我们可以看到，**非均匀分布比均匀分布的熵要小**。现在让我们考虑如何把变量状态的类别传递给接收者。与之前一样，我们可以使用一个 $2bits$ 的数字来完成这件事情。然而，我们可以利用非均匀分布这个特点，**使用更短的编码来描述更可能的事件，使用更长的编码来描述不太可能的事件**。我们希望这样做能够得到一个更短的平均编码长度。我们可以使用下面的编码串（哈夫曼编码）：0、10、110、111来表示状态 {a,b,c,d}{a,b,c,d}。传输的编码的平均长度就是：
$$
average code length = 12×1+14×2+2×18×3=1.75\ bits
$$
这个值与上方的随机变量的熵相等。熵和最短编码长度的这种关系是一种普遍的情形。Shannon 编码定理[https://baike.baidu.com/item/Shannon%20%E7%BC%96%E7%A0%81%E5%AE%9A%E7%90%86/15585931?fr=aladdin](https://baike.baidu.com/item/Shannon 编码定理/15585931?fr=aladdin) 表明**熵是传输一个随机变量状态值所需的比特位下界（最短平均编码长度），底数为2时**。因此，信息熵可以应用在数据压缩方面。这里这篇文章http://www.ruanyifeng.com/blog/2014/09/information-entropy.html讲的很详细了，我就不赘述了。

<br>

## 联合熵

将一维随机变量分布推广到多维随机变量分布，则其**联合熵 (Joint entropy)** 为：
$$
H(X,Y)=-\sum\limits_{x,y}{p(x,y)}{log{p(x,y)}}=-\sum\limits_{i=1}^{n}\sum\limits_{j=1}^{m}{p(x_i,y_i)logp(x_i,y_i)}
$$
**注意点：**

1. 熵只依赖于随机变量的分布，与随机变量取值无关，所以也可以将 $X$ 的熵记作 $H(p)$。
2. 令 $0log0=0$(因为某个取值概率可能为0)。

<br>

## 条件熵 

 条件熵 H(Y|X)H(Y|X) 表示在已知随机变量 XX 的条件下随机变量 YY 的不确定性。条件熵 H(Y|X)H(Y|X) 定义为 XX 给定条件下 YY 的条件概率分布的熵对 XX 的数学期望：
$$
H(Y|X)=\sum\limits_x{p(x)H(Y|X=x)}\\=-\sum\limits_x{p(x)}\sum\limits_y{p(y|x)logp(y|x)}\\=-\sum\limits_x\sum\limits_y{p(x,y)logp(y|x)}\\=-\sum\limits_{x,y}{p(x,y)logp(y|x)}
$$
**条件熵 $H(Y|X)$ 相当于联合熵 $H(X,Y)$ 减去单独的熵 $H(X)$**，即$H(Y|X)=H(X,Y)−H(X)$，证明如下：
$$
H(X,Y)=-\sum\limits_{x,y}{p(x,y)log{p(x,y)}}\\=-\sum\limits_{x,y}{p(x,y)log(p(y|x)p(x))}\\=-\sum\limits_{x,y}{p(x,y)log{p(y|x)}}-\sum\limits_{x,y}{p(x,y)log{p(x)}}\\=H(Y|X)-\sum\limits_{x,y}{p(x,y)log{p(x)}}\\=H(Y|X)-\sum\limits_{x}\sum\limits_{y}{p(x,y)log{p(x)}}\\=H(Y|X)-\sum\limits_{x}{log{p(x)}}\sum\limits_{y}{p(x,y)}\\=H(Y|X)-\sum\limits_{x}{(log{p(x))p(x)}}\\=H(Y|X)-\sum\limits_{x}{p(x)log{p(x)}}\\=H(Y|X)+H(X)
$$
举个例子，比如环境温度是低还是高，和我穿短袖还是外套这两个事件可以组成联合概率分布 $(X,Y)$，因为两个事件加起来的信息量肯定是大于单一事件的信息量的。假设 $H(X)$ 对应着今天环境温度的信息量，由于今天环境温度和今天我穿什么衣服这两个事件并不是独立分布的，所以在已知今天环境温度的情况下，我穿什么衣服的信息量或者说不确定性是被减少了。当已知 $H(X)$ 这个信息量的时候，$H(X,Y)$ 剩下的信息量就是条件熵：$H(Y|X)=H(X,Y)−H(X)$

因此，可以这样理解，**描述 XX 和 YY 所需的信息是描述 XX 自己所需的信息,加上给定 XX 的条件下具体化 YY 所需的额外信息**。关于条件熵的例子可以看这篇文章，讲得很详细。https://zhuanlan.zhihu.com/p/26551798

<br>

## 交叉熵

现在有关于样本集的两个概率分布 $p(x)$ 和 $q(x)$，其中 $p(x)$ 为真实分布， $q(x)$ 非真实分布。如果用真实分布 p(x)p(x) 来衡量识别别一个样本所需要编码长度的期望（平均编码长度）为 $H(p)=∑xp(x)log1p(x)$.如果使用非真实分布 $q(x)$ 来表示来自真实分布 $p(x)$的平均编码长度，则是：$H(p,q)=∑xp(x)log1q(x)$.

**交叉熵刻画的是实际输出概率和期望输出概率的距离**，交叉熵的值越小，则两个概率分布越接近，即实际与期望差距越小。交叉熵中的交叉就体现在 p (期望概率分布), q(实际概率分布)。
$$
H(X) =  - \sum\limits_{i = 1}^K {p({x_i})\log (q({x_i}))}
$$
假如，n=3, 期望输出 $p=(1,0,0)$，模型1的实际输出为 $q_1=(0.5,0.2,0.3)$,模型2的实际输出为 $q_2=(0.8,0.1,0.1)$,那么**交叉熵**为：
$$
H(p,q_1)=-(1 \times {\ln {0.5}} +0 \times ln{0.2} + 0 \times ln{0.3})= 0.69\ nats
$$

$$
H(p,q_2)=-(1 \times {\ln {0.8}} + 0 \times ln{0.1} + 0 \times ln{0.1})= 0.22\ nats
$$

注意：熵的单位随着公式中log运算的底数而变化，**当底数为2时，单位为“比特”(bit)，底数为e时，单位为“奈特”。** 1 nat = 1.44 bits

假如，以“**中国乒乓球队和巴西乒乓球对比赛结果**”为例：

假设中国乒乓球队和巴西乒乓球队历史交手64次，其中中国队获胜63次，63/64是赛前大家普遍认可的中国队获胜概率，这个是先验概率。

那么这次中国队获胜的平均信息量有多大呢？
$$
H(X_i=中国队获胜) =  \frac{{63}}{{64}}{\log _2}\frac{{63}}{{64}}
$$
同理：
$$
H(X_i=巴西队获胜) =  \frac{{1}}{{64}}{\log _2}\frac{{1}}{{64}}
$$
所以，“**中国乒乓球队和巴西乒乓球对比赛结果**”,这一信息的信息熵为：
$$
H(X) =- \sum\limits_{i = 1}^n {p({x_i})\log (p({x_i}))}\\ =H(X_i=中国队获胜)+H(X_i=巴西队获胜)\\ =\frac{{63}}{{64}}{\log _2}\frac{{63}}{{64}} + \frac{{1}}{{64}}{\log _2}\frac{{1}}{{64}}\\ = 0.1164
$$
<br>

## 相对熵（KL散度）

设 $p(x)$、$q(x)$ 是 离散随机变量 $X$ 中取值的两个概率分布，则 $p$ 对 $q$ 的相对熵是：
$$
D_{KL}(p||q)=\displaystyle\sum_{x}p(x)log\frac{p(x)}{q(x)}=E_{p(x)}log\frac{p(x)}{q(x)}
$$
性质：

1. 如果 $p(x)$ 和 $q(x)$ 两个分布相同，那么相对熵等于0；
2. $DKL(p||q)≠DKL(q||p)$ ，相对熵具有不对称性；
3. $D_{KL}(p||q)\geq0$

我们再化简一下相对熵的公式。
$$
DKL(p||q)=∑xp(x)logp(x)q(x)=∑xp(x)logp(x)−p(x)logq(x)
$$
有没有发现什么？

熵的公式 $H(p)=−∑xp(x)logp(x)$

交叉熵的公式 $H(p,q)=∑xp(x)log1q(x)=−∑xp(x)logq(x)$

所以有：==$DKL(p||q)=H(p,q)−H(p)$== 当用非真实分布 $q(x)$ 得到的平均码长比真实分布 $p(x)$ 得到的平均码长多出的比特数就是相对熵）。又因为 $DKL(p||q)≥0$ 所以 $H(p,q)≥H(p)$（当 $p(x)=q(x)$ 时取等号，此时交叉熵等于信息熵）

在机器学习中，训练数据分布是固定的，也就是 $H(p)$ 为常量，**最小化相对熵等价于最小化交叉熵也等价于最大化似然估计**。其次，我们没有真实数据的分布，所以只能希望模型学到的分布 $P(model)$ 和训练数据的分布 $P(train)$ 尽量相同。假设训练数据是从总体中独立同分布采样的，那么我们可以通过最小化训练数据的经验误差来降低模型的泛化误差。

<br>

## 总结

1. 信息熵是**衡量随机变量分布的混乱程度，是随机分布各事件发生的信息量的期望值**，随机变量的取值个数越多，状态数也就越多，信息熵就越大，混乱程度就越大。当随机分布为均匀分布时，熵最大。
2. 信息熵是传输一个随机变量状态值所需的比特位下界（**最短平均编码长度**）。
3. 信息熵**推广到多维领域，则可得到联合信息熵**。
4. 条件熵表示的是在 $X$ 给定条件下，$Y$ 的条件概率分布的熵对 $X$的期望。
5. 相对熵可以用来衡量两个概率分布之间的差异，是指用 $q$ 来表示分布 $p$ 额外需要的编码长度，相对熵等于交叉熵减去自身的熵。
6. 交叉熵可以来衡量在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要付出的努力的大小，是指用分布 $q$ 来表示本来表示分布 $p$ 的平均编码长度。

<br>

## 交叉熵损失函数

为什么Cross Entropy损失函数常用于分类问题中呢？我们从一个简单的例子来分析。

> **问题引入**：
> 假设我们有一个**三分类**问题，分别用**模型1**和**模型2**来进行预测。

结果如下：

- 模型1

| 样本id | 预测值        | 实际值  | 是否预测正确 |
| ------ | ------------- | ------- | ------------ |
| 1      | [0.3,0.3,0.4] | [0,0,1] | √            |
| 2      | [0.3,0.4,0.3] | [0,1,0] | √            |
| 3      | [0.1,0.2,0.7] | [1,0,0] | ×            |

- 模型2

| 样本id | 预测值        | 实际值  | 是否预测正确 |
| ------ | ------------- | ------- | ------------ |
| 1      | [0.1,0.2,0.7] | [0,0,1] | √            |
| 2      | [0.1,0.7,0.2] | [0,1,0] | √            |
| 3      | [0.3,0.4,0.3] | [1,0,0] | ×            |

对**样本1和样本2**，**模型1**以0.4>0.3的微弱优势正确预测样本1的标签为**类别3**，而**模型2**以0.7>0.2>0.1的巨大优势毫无悬念的正确预测出样本标签。

对于**样本3**，**模型1**和**模型2**均预测错误，但**模型1**以0.7>0.2>0.1的概率，错误的预测样本标签为**标签3**,但实际标签为**标签1**，错的离谱！！！但**模型2**虽然也预测错了，但0.4>0.3>0.2还不算太离谱。

现在我们用损失函数来定义模型的表现。

<br>

#### Classification Error（分类错误率）

$$
ClassificationError=\frac{count of error items}{count of all items}
$$

-**模型1**：
$$
ClassificationError=\frac{1}{3}
$$
-**模型2**：
$$
ClassificationError=\frac{1}{3}
$$
模型1和模型2虽然都预测错了1个，但相对来说模型2表现更好，按理说模型越好，损失函数值越小，但分类错误率并没表现出来。

#### Mean Squared Error（均方误差）

均方误差损失也是一种比较常见的损失函数，其定义为：
$$
MSE=\frac{1}{n}\sum\limits_i^n {{{(\widehat {{y_i}} - {y_i})}^2}}
$$
**模型1**：
$$
MSE=\frac{(0.3^2+0.3^2+0.6^2)+(0.3^2+0.6^2+0.3^2)+(0.9^2+0.2^2+0.7^2)}{3}
\ =0.81
$$
-**模型2**：
$$
MSE=\frac{(0.1^2+0.1^2+0.3^2)+(0.1^2+0.3^2+0.2^2)+(0.7^2+0.4^2+0.3^2)}{3}
\ =0.34
$$
我们发现MSE能判断出模型2优于模型1，但采用梯度下降法来求解的时候，MSE的一个缺点就是其偏导值在输出概率值接近0或者接近1的时候非常小，这可能会造成模型刚开始训练时，梯度几乎消失。

<img src="/images/posts/Entropy/2.png" />

对于分类问题的损失函数来说，分类错误率（分类精确率）和平方和损失都不是很好的损失函数，下面我们来看一下交叉熵损失函数的表现情况。

#### Cross Entropy（交叉熵损失函数）

* **定义**

  ​		在二分类的情况下，模型最终预测的结果只有2类，对于每个类别我们预测的概率为和。
  此时Binary Cross Entropy：==$J=-[y·log(p)+(1-y)·log(1-p)]$==. 其中 y 为样本标签，正样本标签为1，负样本标签为0；p 为预测为正样本的概率。

  ​		多分类实际是二分类的扩展。==$J= - \sum\limits_{i = 1}^K {y_i\log ({p_i})}$==. 其中 K 为类别的数量，y 指是否是类别，p 为样本属于类别的概率。

  ​		现在我们用交叉熵损失函数来计算损失函数值。

  ​		模型1：$CEE=-[0 \times log0.3 + 0 \times log0.3 + 1 \times log0.4] -[0 \times log0.3 + 1 \times log0.4 + \\ 0 \times log0.3] -[1 \times log0.1 + 0 \times log0.2 + 0 \times log0.7]\\= 0.397+0.397 +1 =1.8$

  ​		模型2：$CEE=-[0 \times log0.1 + 0 \times log0.2 + 1 \times log0.7]-[0 \times log0.1 + 1 \times log0.7 + \\ 0 \times log0.2]-[1 \times log0.1 + 0 \times log0.2 + 0 \times log0.3]\\= 0.15+0.15+0.397=0.697$

  ​		交叉熵损失函数可以捕捉到模型1和模型2的差异。

* **函数性质**

  ​		交叉熵损失函数经常用于分类问题中，特别是神经网络分类问题，由于交叉熵涉及到计算每个类别的概率，所以在神经网络中，交叉熵与 softmax 函数紧密相关。
  ​		我们用神经网络的最后一层输出情况来看。
  <img src="/images/posts/Entropy/3.png"/>

下面我们来推下整个求导公式，求导如图所示，分为三个过程：
$$
\frac{{\partial J}}{{\partial {w_n}}} = \frac{{\partial J}}{{\partial {p_j}}} \cdot \frac{{\partial {p_j}}}{{\partial {y_i}}} \cdot \frac{{\partial {y_i}}}{{\partial {w_n}}}
$$
先看 $\frac{{\partial J}}{{\partial {p_j}}}$:
$$
\frac{\partial J}{\partial p_j} = \frac{\partial (-\sum\limits_{j=1}^{k}{y_i}{\log ({p_j})})}{\partial p_j} = -\sum_{j=1}^{k}{\frac{y_i}{p_j}}
$$


结合上图，再看 $\frac{{\partial {p_j}}}{{\partial {y_i}}}$:
$$
{p_j} = \frac{{{e^{{y_i}}}}}{\sum\limits_{j = 1}^K {{e^{{y_j}}}} }
$$

- 当 $j=i$ 时候:
  $$
  {p_i} = \frac{e^{y_i}}{\sum\limits_{j=1}^{K}{e^{y_i}}}
  $$

$$
\frac{\partial {p_j}}{\partial {y_i}} 

= \frac{\partial({\frac{e^{y_i}}{\sum\limits_{j=1}^{K}{e^{y_j}}}})}{\partial{y_i}}

=\frac{(e^{y_i})'{\sum\limits_{j=1}^K{e^{y_i}}} - {e^{y_i}}({\sum\limits_{j=1}^K{e^{y_k}}})'}{(\sum\limits_{j=1}^K{e^{y_i}})^2}

= \frac{{e^{y_i}} {\sum\limits_{j = 1}^K {e^{y_i}}  - ({e^{y_i}})^2}{(\sum\limits_j=1}^K {e^{{y_i}}}})^2

= \frac{e^{y_i}}{\sum\limits_{j=1}^K{e^{y_i}}} - \frac{({e^{y_i}})^2}{(\sum\limits_{j=1}^{K}{e^{y_i}})^2}

= \frac{e^{y_i}}{\sum\limits_{j=1}^K{e^{y_i}}}(1-\frac{e^{y_i}}{\sum\limits_{j=1}^K{e^{y_i}}})

= S({y_i})(1 - S({y_i})

=p(i)(1-p(i))
$$

- 当 $j≠i$ 时候:
  
  ...
  
  
$$
  \frac{{\partial {y_i}}}{{\partial {w_n}}}=x_n
  $$
  接下来我们只需要把上面的组合起来：
  $$
  \frac{{\partial J}}{{\partial {p_j}}} \cdot \frac{{\partial {p_j}}}{{\partial {y_i}}} \begin{array}{l} =  - p(i)(1 - p(i))\sum\limits_{i = 1,i = j}^K {\frac{{{y_i}}}{{{p_i}}}}  - p(i)p(j)\sum\limits_{i = 1,i \ne j}^K {\frac{{{y_i}}}{{{p_j}}}}=  - (1 - p(i))\sum\limits_{i = 1,i = j}^K {{y_i}}  - p(i)\sum\limits_{i = 1,i \ne j}^K {{y_i}}=  - \sum\limits_{i = 1,i = j}^K {{y_i}}  + p(i)\sum\limits_{i = 1}^K {{y_i}}\end{array}
  $$
  
  
  最后针对分类问题，给定的中只会有一个类别是1，其他类别都是0，所以
  $$
  \frac{{\partial J}}{{\partial {p_j}}} \cdot \frac{{\partial {p_j}}}{{\partial {y_i}}}  \cdot \frac{{\partial y_i}}{{\partial {w_n}}} = \frac{{\partial {J}}}{{\partial {w_n}}}=(p(i)-1)x_i
  $$

注意看 $p(i)-1$，是啥？是不是**SoftMax层的输出的概率-1**，梯度就是这么容易计算！！！

而且梯度下降已经不与 sigmoid 的导数相关了，而是由误差来影响，当误差较大时则下降较快，让梯度下降法更有效率，避免了训练慢的问题。就是为什么神经网络分类器要用交叉熵损失函数的原因。

<br>

## SOFTMAX 函数

SOFTMAX 长什么样子？如下图所示

![image_1cq5pgbtq6vsb0ks2a1jippcf9.png](https://segmentfault.com/img/bVbkP8E?w=507&h=736)

从图的样子上看，和普通的全连接方式并无差异，但激励函数的形式却大不一样。
![微信截图_20181022153759.png-66.5kB](https://segmentfault.com/img/remote/1460000017321270?w=809&h=408)
![image_1cqdbm3l51ntme1g1i4pnd1u2t4c.png-186.3kB](https://segmentfault.com/img/remote/1460000017321271?w=678&h=395)

- 首先后面一层作为预测分类的输出节点，每一个节点就代表一个分类，如图所示，那么这7个节点就代表着7个分类的模型，任何一个节点的激励函数都是：
  $$
  {\sigma _i}(z) = \frac{{e^{z_i}}}{\sum\limits_{j = 1}^m {e^{z_i}}}
  $$

其中就是节点的下标次序，而 $z_i=w_i+b_i$ 也就说这是一个线性分类器的输出作为自然常数的指数。最有趣的是最后一层有这样的特性：
$$
\sum\limits_{i = 1}^J {\sigma _i}(z)  = 1
$$
也就是说最后一层的每个节点的输出值的加和是1。这种激励函数从物理意义上可以解释为一个样本通过网络进行分类的时候在每个节点上输出的值都是小于等于1的，是它从属于这个分类的概率。
训练数据由训练样本和分类标签组成。如下图所,j假设有7张图，分别为飞机、汽车、轮船、猫、狗、鸟、太阳，则图像的分类标签如下表示：
$$
\left[ \begin{array}{l}1 \\0 \\0 \\0 \\0 \\0 \\0 \end{array} \right]\left[ \begin{array}{l}0 \\1 \\0 \\0 \\0 \\0 \\0 \end{array} \right]\left[ \begin{array}{l}0 \\0 \\1 \\0 \\0 \\0 \\0 \end{array} \right]\left[ \begin{array}{l}0\\0 \\0 \\1 \\0 \\0 \\0 \end{array} \right]\left[ \begin{array}{l}0\\0 \\0 \\0 \\1 \\0 \\0 \end{array} \right]\left[ \begin{array}{l}0\\0 \\0 \\0 \\0 \\1 \\0 \end{array} \right]\left[ \begin{array}{l}0\\0 \\0 \\0 \\0 \\0 \\1 \end{array} \right]
$$
这种激励函数通常用在神经网络的最后一层作为分类器的输出，有7个节点就可以做7个不同类别的判别，有1000个节点就可以做1000个不同样本类别的判断。

<br>